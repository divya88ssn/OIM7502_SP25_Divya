{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9383bafa",
   "metadata": {},
   "source": [
    "### Prompt Engineering Playground üöÄ\n",
    "\n",
    "This notebook demonstrates **prompt engineering** using **Hugging Face LLMs** and **Gradio**. You'll learn how to:\n",
    "\n",
    "‚úÖ Explore different prompt types  \n",
    "‚úÖ Control LLM outputs with parameters (temperature, top-p, max tokens)  \n",
    "‚úÖ Build an interactive **Gradio app** for live prompt experimentation  \n",
    "\n",
    "By the end of this notebook, you'll have an **interactive playground** you can deploy on **Hugging Face Spaces**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902fe3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading distilgpt2 model...\n",
      "‚úÖ distilgpt2 loaded successfully on CPU.\n",
      "üîÑ Loading flan-t5-small model...\n",
      "‚úÖ flan-t5-small loaded successfully on CPU.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# ===================================================\n",
    "# STEP 1: Load Models and Tokenizers (on CPU)\n",
    "# ===================================================\n",
    "print(\"üîÑ Loading distilgpt2 model...\")\n",
    "distilgpt2_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "distilgpt2_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "distilgpt2_model.to(\"cpu\")  # Explicitly move to CPU\n",
    "print(\"‚úÖ distilgpt2 loaded successfully on CPU.\")\n",
    "\n",
    "print(\"üîÑ Loading flan-t5-small model...\")\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "flan_model.to(\"cpu\")  # Explicitly move to CPU\n",
    "print(\"‚úÖ flan-t5-small loaded successfully on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aeb4754-e730-4804-baa7-3f2381df494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Divya Gunasekaran\\AppData\\Local\\Temp\\ipykernel_9136\\4267715357.py:77: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========================\n",
    "# Generation Function\n",
    "# ===========================\n",
    "def chat_with_model(message, history, model_name, max_tokens=200, temperature=0.7, top_p=0.9):\n",
    "    # Construct the conversation history as a single prompt\n",
    "    conversation = \"\"\n",
    "    for user_msg, bot_msg in history:\n",
    "        conversation += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
    "    \n",
    "    # Append current user message\n",
    "    conversation += f\"User: {message}\\nAssistant:\"\n",
    "\n",
    "    # Choose the model\n",
    "    if model_name == \"distilgpt2\":\n",
    "        tokenizer = distilgpt2_tokenizer\n",
    "        model = distilgpt2_model\n",
    "\n",
    "        # Tokenize conversation history + latest user message\n",
    "        inputs = tokenizer(conversation, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cpu\")\n",
    "\n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        # Decode generated text\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # We want to extract **just the reply**, not the full history!\n",
    "        # (Simple splitting logic, can be made smarter)\n",
    "        response = response[len(conversation):].strip()\n",
    "\n",
    "    elif model_name == \"flan-t5-small\":\n",
    "        tokenizer = flan_tokenizer\n",
    "        model = flan_model\n",
    "\n",
    "        # Prompt for flan: explicit task format\n",
    "        prompt = conversation  # Flan expects instruction/task format\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cpu\")\n",
    "\n",
    "        # Generate response\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    else:\n",
    "        response = \"‚ö†Ô∏è Model not recognized.\"\n",
    "\n",
    "    # Update history\n",
    "    history.append((message, response))\n",
    "    return history, history\n",
    "\n",
    "# ===========================\n",
    "# Gradio Interface\n",
    "# ===========================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ü§ñ Prompt Engineering Chatbot with Context Memory\")\n",
    "\n",
    "    # Model selector\n",
    "    model_selector = gr.Dropdown(choices=[\"distilgpt2\", \"flan-t5-small\"], value=\"distilgpt2\", label=\"Choose Model\")\n",
    "\n",
    "    # Chatbot component (displays conversation)\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    # User input field\n",
    "    message_input = gr.Textbox(label=\"Your Message\", placeholder=\"Ask me anything...\")\n",
    "\n",
    "    # Generation controls\n",
    "    with gr.Row():\n",
    "        max_tokens_slider = gr.Slider(10, 512, value=200, step=10, label=\"Max Tokens\")\n",
    "        temperature_slider = gr.Slider(0.1, 1.5, value=0.7, step=0.1, label=\"Temperature\")\n",
    "        top_p_slider = gr.Slider(0.1, 1.0, value=0.9, step=0.1, label=\"Top-p\")\n",
    "\n",
    "    # Clear chat button\n",
    "    clear_button = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    # State to hold conversation history\n",
    "    state = gr.State([])\n",
    "\n",
    "    # Events\n",
    "    message_input.submit(\n",
    "        fn=chat_with_model,\n",
    "        inputs=[message_input, state, model_selector, max_tokens_slider, temperature_slider, top_p_slider],\n",
    "        outputs=[chatbot, state]\n",
    "    )\n",
    "\n",
    "    clear_button.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "# Launch app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba64505-a383-4dce-a956-50023c1d0f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
